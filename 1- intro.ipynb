{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b505fee2",
   "metadata": {},
   "source": [
    "# <span style=\"color: rgb(178, 91, 9); letter-spacing: 1px; word-spacing: 5px\">Comparative Analysis of ML Algorithms For Skin Cancer Classification</span>\n",
    "<hr style=\"background-color:rgb(178, 91, 9); height:2px\">\n",
    "\n",
    "\n",
    "## __Table of Contents:__\n",
    "1. [Introduction](#Introduction)\n",
    "    1. [Project Details](#Project-Details)  \n",
    "        1. [Topic Elaboration](#Topic-Elaboration)\n",
    "        2. [Melanoma Explanation](#Melanoma-Explanation)\n",
    "    2. [Team Details](#Team-Details)  \n",
    "    3. [Dataset Description](#Dataset-Description)\n",
    "    4. [Methodology](#Methodology)\n",
    "        1. [Collection](#Collection)\n",
    "        2. [Analysis](#Analysis)\n",
    "        3. [No Sampling](#No-Sampling)\n",
    "        4. [Over sampling](#Over-Sampling)\n",
    "        5. [Under sampling](#Under-Sampling)\n",
    "        6. [Combination Sampling](#Combine-Sampling)\n",
    "    5. [Model Training](#Model-Trainging)\n",
    "        1. [Classification Models](#Classification-Models)\n",
    "        2. [Ensemble Learner](#Ensemble-Learner)\n",
    "\n",
    "\n",
    "# Introduction       \n",
    "\n",
    "\n",
    "## Project Details\n",
    "\n",
    "### Topic Elaboration \n",
    "Skin cancer is a serious disease that can be fatal if not detected and treated early. Machine learning algorithms have been used to develop models that can assist in the early detection of skin cancer. In this project, we will conduct a comparative analysis of various machine learning algorithms for skin cancer detection. The first step in this analysis is to collect a dataset and preprocess it. Once we have selected the algorithms, we can train them using the extracted features and the training dataset. This involves tuning the hyperparameters of the models to achieve optimal performance. The models are then evaluated using the testing dataset, which includes metrics such as accuracy, precision, recall,confusion matrix and F1 score. \n",
    "\n",
    "### Melanoma\n",
    "Melanoma is a type of skin cancer that arises from the pigment-producing cells in the skin called melanocytes. It is the deadliest form of skin cancer, with increasing incidence worldwide. Melanoma usually appears as a new growth or changes to an existing mole on the skin, commonly on the back, legs, arms, and face. Risk factors for melanoma include exposure to UV radiation, history of sunburns, fair skin, family history of melanoma, and weakened immune system.\n",
    "\n",
    "Melanoma may present various signs and symptoms such as asymmetry, irregular borders, color variation, and a diameter greater than 6 millimeters. The disease can spread to other parts of the body and become life-threatening if left untreated.\n",
    "\n",
    "Melanoma treatment typically involves surgical removal of the tumor, followed by other treatment options such as radiation therapy or chemotherapy. Early detection and treatment are crucial for successful outcomes, and prevention measures such as regular skin exams and sun protection are important to reduce the risk of developing melanoma.\n",
    "\n",
    "\n",
    "## Team Details\n",
    "Our team comprises of 4 members(BTech CSE, 6th Semester):\n",
    "1. Suvanjana Brahma- 20051856\n",
    "2. Rohit Kumar Singh- 20051880\n",
    "3. Bhanu Pratap Singh-20051881\n",
    "4. Ayush Priyam-2001933\n",
    "\n",
    "\n",
    "## Dataset Description\n",
    "For our project, we are using a SIIM-ISIC Melanoma Classification dataset, from which the features had been extracted. The original dataset consists of a total of 33,126 skin lesion images, with 32,278 images in the training set and 848 images in the test set. The images were obtained from various sources, such as dermoscopy and clinical photography, and labeled as either malignant or benign by dermatologists. But since we are using only ML algorithms so we are working on the preprocessed csv file dataset. Our dataset typically consists of a set of numerical features that represent various characteristics of the skin lesions, such as texture, color, and shape. There are around 30,000 rows and 40 columns in our dataset.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "### Collection\n",
    "\n",
    "### Analysis\n",
    "\n",
    "\n",
    "### No Sampling\n",
    "In this the orginal dataset is used without add new rows. Here we are using multiple models to get result and then evaluating them on basis of accuracy, precision, F1 score etc\n",
    "\n",
    "### Over Sampling\n",
    "Oversampling is a technique used in machine learning to address class imbalance by generating synthetic samples of the minority class. It improves the performance of models by creating a more balanced training dataset. Techniques such as SMOTE and ADASYN are commonly used to generate synthetic samples. However, oversampling can also introduce the risk of overfitting to the minority class and poor generalization performance. Therefore, it is important to carefully evaluate the performance of machine learning models trained with oversampled data and consider other approaches such as undersampling.\n",
    "Here we are using 3 types of over Sampling techniques, they are:-\n",
    "1. <b><ins>Random Over Sampler:-</ins></b><br>\n",
    "Random over-sampling is a type of oversampling technique used in machine learning to address class imbalance. In this technique, synthetic samples of the minority class are generated by randomly duplicating existing samples until the minority class is balanced with the majority class. Random over-sampling is a simple and straightforward technique that can be effective in some cases, but it may also lead to overfitting if the synthetic samples are too similar to the original samples.\n",
    "2. <b><ins>SMOTE (Synthetic Minority Over-sampling Technique):-</ins></b><br>\n",
    "SMOTE is an oversampling technique used in machine learning to address class imbalance. It generates synthetic samples of the minority class by interpolating between existing minority class samples. SMOTE creates synthetic samples that are more diverse and realistic, and less prone to overfitting than other oversampling techniques. However, it may also generate noisy or irrelevant samples if the distance metric used is not appropriate or if the minority class is highly overlapping with the majority class. \n",
    "3. <b><ins>ADASYN (Adaptive Synthetic Sampling):-</ins></b><br>\n",
    "ADASYN (Adaptive Synthetic Sampling) is a data oversampling technique used in machine learning to address imbalanced datasets. It generates synthetic samples for the minority class(es) by adaptively generating more synthetic samples for the minority class samples that are difficult to learn. ADASYN computes the density distribution of the minority class samples and generates synthetic samples for the minority class samples with lower density. It uses a K-nearest neighbors algorithm to generate synthetic samples between the K nearest neighbors. ADASYN is computationally efficient, handles high-dimensional datasets and has been shown to outperform other oversampling techniques in various applications, including fraud detection and medical diagnosis.\n",
    "\n",
    "### Under Sampling\n",
    "Undersampling is a method employed in machine learning and data analysis to deal with the issue of imbalanced datasets. When a dataset is imbalanced, one class may have a considerably larger number of examples than the other class(es), which can lead to inadequate performance of the machine learning algorithm in predicting the minority class. To balance the dataset, undersampling involves reducing the number of examples in the majority class.Here we are using 4 types of over Sampling techniques, they are:-\n",
    "1. <b><ins>AIKNN:-</ins></b><br>\n",
    "AIKNN (Adaptively Indexed k-Nearest Neighbor) is a modified version of the K-Nearest Neighbor algorithm used for classification and regression tasks in machine learning. It assigns weights to samples based on their local density to address imbalanced datasets. By grouping samples into different density regions and assigning weights based on the density of samples, AIKNN ensures that each sample is given the appropriate importance in the algorithm. It is used in various applications such as medical diagnosis and image recognition and has been shown to perform better than other algorithms in imbalanced datasets.\n",
    "2. <b><ins>Nearmiss:-</ins></b><br>\n",
    "NearMiss is an undersampling technique used in machine learning to handle imbalanced datasets. It aims to balance the dataset by reducing the number of samples in the majority class(es). NearMiss identifies the samples from the majority class that are closest to the minority class based on a distance metric and removes them from the dataset. This ensures that the selected samples represent the majority class and maintain the minority class characteristics. NearMiss is a straightforward and computationally efficient technique that is effective in addressing imbalanced datasets in various applications such as fraud detection and medical diagnosis.\n",
    "3. <b><ins>IHT:-</ins></b><br>\n",
    "IHT (Iterative Hard Thresholding) is a sparse signal recovery algorithm used in machine learning for feature selection and signal recovery. It removes components with small magnitudes iteratively until a sparse solution is obtained. IHT relies on the sparsity assumption that signals or features are usually composed of only a few non-zero components. It is a computationally efficient algorithm that can handle large datasets and is used in various applications, including image and speech processing, bioinformatics, and compressive sensing.\n",
    "4. <b><ins>ENN:-</ins></b><br>\n",
    "ENN (Edited Nearest Neighbors) is a data cleaning technique used in machine learning to address imbalanced datasets. It is an undersampling method that aims to balance the dataset by removing noisy samples from the majority class(es). ENN identifies misclassified samples in the majority class by their nearest neighbors and removes them from the dataset. This simple and computationally efficient technique can be combined with other sampling techniques to enhance the performance of the machine learning algorithm. ENN is used in various applications such as medical diagnosis, credit scoring, and image recognition, and it has been shown to improve the performance of the machine learning algorithm in imbalanced datasets.\n",
    "\n",
    "### Combination Sampling\n",
    "Combining oversampling and undersampling techniques is a method used to tackle imbalanced datasets in machine learning. The approach involves creating additional samples for the minority class and reducing samples for the majority class to balance the dataset. The goal is to improve machine learning algorithm performance in predicting the minority class. Different techniques like random oversampling and synthetic minority oversampling technique (SMOTE) can be used. The specific technique chosen depends on the dataset and machine learning algorithm used.Here we are using 3 types of over Sampling techniques, they are:-\n",
    "1. <b><ins>SMOTEENN:-</ins></b><br>\n",
    "SMOTEEEN is an extension of the SMOTE algorithm that addresses the issue of imbalanced data in classification problems. It generates synthetic samples for the minority class by combining SMOTE and Edited Nearest Neighbors (ENN) algorithms, helping to balance the distribution of classes in imbalanced datasets and reducing the risk of generating noisy synthetic samples. SMOTEEEN has been shown to improve the performance of machine learning models on imbalanced datasets in various applications, including medical diagnosis and credit risk assessment.\n",
    "2. <b><ins>SMOTETomek:-</ins></b><br>\n",
    "SMOTEomek is a hybrid algorithm that combines SMOTE and Tomek links to address the issue of imbalanced datasets. SMOTE generates synthetic samples for the minority class, while Tomek links remove overlapping samples from both the minority and majority classes. This results in a more balanced and cleaner dataset that can be used to train machine learning models. SMOTEomek has been shown to be effective in improving the classification accuracy of imbalanced datasets in various applications, including medical diagnosis and credit risk assessment.\n",
    "3. <b><ins>ROS-AllKNN:-</ins></b><br>\n",
    "ROSKNN is a machine learning algorithm that combines random over-sampling and the k-Nearest Neighbors (KNN) algorithm to handle imbalanced datasets in classification problems. The algorithm generates synthetic samples by randomly duplicating existing minority class samples and then classifies them using the KNN algorithm. This helps to balance the dataset and ensure that synthetic samples are generated in regions of the feature space where they are likely to belong. ROSKNN has been shown to be effective in improving the performance of machine learning models on imbalanced datasets in various applications, including medical diagnosis and fraud detection. It is a simple and straightforward technique that can be easily applied to imbalanced datasets.\n",
    "\n",
    "### Classification Models\n",
    "Supervised and unsupervised learning are two major types of machine learning techniques used to train models for predictive modeling, pattern recognition, and other tasks. In our project we are using supervised learning. Supervised learning is a type of machine learning where the algorithm is trained on a labeled dataset. The labeled dataset contains input data and corresponding output data or labels. During the training phase, the algorithm learns to map the input data to the correct output by adjusting its internal parameters. Once the model is trained, it can be used to make predictions on new, unlabeled data. Examples of supervised learning include classification and regression problems.\n",
    "The four classification models used in this project are:-\n",
    "1. <b><ins>Logistic Regression:-</ins></b><br>\n",
    "Logistic regression is a statistical method used for binary classification problems. It uses a logistic function to model the relationship between input variables and binary outcomes. The model is trained on a labeled dataset to predict probabilities of outcomes, which can be converted to binary predictions. It is popular for its simplicity and interpretability.\n",
    "2. <b><ins>Decision Tree Classifier:-</ins></b><br>\n",
    "Decision tree classifier is a supervised learning algorithm used for classification problems. It recursively partitions the input space based on input feature values until a stopping criterion is met. It is interpretable, can handle numerical and categorical features, but may suffer from overfitting.\n",
    "3. <b><ins>Support Vector Machine:-</ins></b><br>\n",
    "Support vector machines (SVMs) are a class of supervised learning algorithms used for classification and regression analysis. They work by finding the optimal hyperplane that separates the two classes, using a loss function to penalize misclassifications and to maximize the margin. SVMs are strong in theory and can handle non-linear boundaries, but can be computationally expensive.\n",
    "4. <b><ins>K-Nearest Neighbours:-</ins></b><br>\n",
    "K-nearest neighbors (KNN) is a supervised learning algorithm used for classification and regression. It determines the class or value of an input sample by the classes or values of its K nearest neighbors in the training data, using a distance metric such as Euclidean distance. K is an important hyperparameter and the algorithm requires careful selection of distance metric and data normalization. It is simple, effective and can handle non-linear boundaries, but can be computationally expensive.\n",
    "\n",
    "\n",
    "\n",
    "### Ensemble Learner\n",
    "Ensemble learning is a machine learning technique that involves combining multiple models to improve the overall performance of a learning algorithm. It is based on the idea that combining multiple models can often result in better predictions than using a single model alone. The models can be combined in various ways, such as through averaging, voting, or stacking. Ensemble learning is commonly used in tasks such as classification, regression, and anomaly detection, and has been successful in many real-world applications.<br><br>\n",
    "For our project we are using below two types of esnemble learner techniques:-<br><br> \n",
    "1. <b><ins>Bagging (Random Forest):-</ins></b><br>\n",
    "Bagging involves training multiple instances of the same base model on different subsets of the training data, selected with replacement (bootstrapping). The predictions of each model are then combined, typically by averaging or voting, to produce the final prediction. This approach can reduce overfitting and improve the stability of the model.Here we are using Random forest. It is a popular ensemble method that combines bagging with decision trees. In Random Forest, multiple decision trees are trained on different subsets of the training data, and the final prediction is made by averaging the predictions of all the trees.\n",
    "2. <b><ins>Boosting:-</ins></b><br> \n",
    "Boosting involves training a sequence of models where each model tries to correct the errors of the previous models. In boosting, each subsequent model is trained on the examples that were misclassified by the previous models. This approach can improve the accuracy of the model and is particularly effective for reducing bias. Types of boosting we are concerned with in our project:-<br>\n",
    "        \n",
    "    * Adaboost (Adaptive Boosting):- Adaboost is a popular boosting method that assigns weights to each example in the training data, with higher weights for examples that were misclassified by the previous models. The subsequent models are then trained on the weighted data, with more emphasis on the misclassified examples.\n",
    "    * LightGBM (Light Gradient Boosting Machine):- LightGBM is another optimized implementation of Gradient Boosting that uses a histogram-based approach to reduce the memory usage and improve the training speed of the model.\n",
    "    * CatBoost (Categorical Boosting):- CatBoost is a boosting method that is specifically designed to handle categorical features in the data. It uses an ordered boosting algorithm that considers the order of the categorical values when constructing the decision trees.\n",
    "    * XGBoost (Extreme Gradient Boosting):- XGBoost is an optimized implementation of Gradient Boosting that uses a combination of parallel processing, regularization, and hardware optimization to improve the training speed and accuracy of the model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
